{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"MS_HW3 v2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"8zCktvTvWdNf"},"source":["# ECE-6524 / CS-6524 Deep Learning\n","# Assignment 3 [100 pts]\n","\n","In this assignment, **you need to complete the Yolo loss function, and train an object detector. Yay!**\n","\n","## Submission guideline for the coding part (Jupyter Notebook)\n","\n","1. Click the Save button at the top of the Jupyter Notebook\n","2. Please make sure to have entered your Virginia Tech PID below\n","3. Once you've completed everything (make sure output for all cells are visible), select File -> Download as -> PDF via LaTeX\n","4. Look at the PDF file and make sure all your solutions are displayed correctly there \n","7. Zip all the files along with this notebook (Please don't include the data). Name it as Assignment_3_Code_[YOUR PID NUMBER].zip\n","8. Name your PDF file as Assignment_2_NB_[YOUR PID NUMBER].pdf\n","9. **<span style=\"color:blue\"> Submit your zipped file and the PDF SEPARATELY**</span>\n","\n","Note: if facing issues with step 3 refer: https://pypi.org/project/notebook-as-pdf/\n","\n","## Submission guideline for the coding part (Google Colab)\n","\n","1. Click the Save button at the top of the Notebook\n","2. Please make sure to have entered your Virginia Tech PID below\n","3. Follow last two cells in this notebook for guidelines to download pdf file of this notebook\n","4. Look at the PDF file and make sure all your solutions are displayed correctly there \n","5. Zip all the files along with this notebook (Please don't include the data). Name it as Assignment_2_Code_[YOUR PID NUMBER].zip\n","6. Name your PDF file as Assignment_2_NB_[YOUR PID NUMBER].pdf\n","7. **<span style=\"color:blue\"> Submit your zipped file and the PDF SEPARATELY**</span>\n","\n","**While you are encouraged to discuss with your peers, <span style=\"color:blue\">all work submitted is expected to be your own.</span> <span style=\"color:red\">If you use any information from other resources (e.g. online materials), you are required to cite it below you VT PID. Any violation will result in a 0 mark for the assignment.</span>**"]},{"cell_type":"markdown","metadata":{"id":"IV0Y0mTdWdNh"},"source":["### Please Write Your VT PID Here: 906255893\n","### Reference (if any): \n","\n","https://araintelligence.com/blogs/deep-learning/object-detection/yolo_v1/\n","\n","https://www.kdnuggets.com/2018/05/implement-yolo-v3-object-detector-pytorch-part-1.html\n","\n","https://github.com/pjreddie/darknet/issues/558\n","\n","https://stats.stackexchange.com/questions/333968/understanding-the-yolo-algorithm\n","\n","https://zhuanlan.zhihu.com/p/139713442\n","\n","https://github.com/weiaicunzai/Pytorch-yolo/blob/master/YOLOLoss.py\n","\n","https://github.com/z-huabao/pytorch-yolov1/blob/master/yoloLoss.py\n","\n","https://github.com/Eversee22/yolov1-pytorch/blob/master/loss.py\n","\n","https://github.com/motokimura/yolo_v1_pytorch/blob/master/loss.py\n","\n","https://github.com/xiongzihua/pytorch-YOLO-v1/blob/master/yoloLoss.py"]},{"cell_type":"markdown","metadata":{"id":"MtAdCKnHWdNi"},"source":["In this homework, you would need to use **Python 3.6+** along with the following packages:\n","```\n","1. pytorch 1.2\n","2. torchvision\n","3. numpy\n","4. matplotlib\n","5. tqdm (for better, cuter progress bar. Yay!)\n","```\n","To install pytorch, please follow the instructions on the [Official website](https://pytorch.org/). In addition, the [official document](https://pytorch.org/docs/stable/) could be very helpful when you want to find certain functionalities. \n","\n","\n","<span style=\"color:red\">Note that, on a high-end GPU, it sill takes 3-4 hours to train. **SO START EARLY. IT'S IMPOSSIBLE TO FINISH IT AT THE LAST MINUTE!**</span>\n"]},{"cell_type":"markdown","metadata":{"id":"V6jpDoOPWdNj"},"source":["### Colab Setup"]},{"cell_type":"markdown","metadata":{"id":"AJq2rCu0WdNl"},"source":["### To select GPU in Google Colab:\n","- go to **Edit -> Notebook settings -> Hardware accelerator -> GPU**"]},{"cell_type":"code","metadata":{"id":"llpEC27VWdNo","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"19f56c69-d48f-4e36-d69a-ea242c884435"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_9igmFYLWdNu"},"source":["import sys\n","# modify \"customized_path_to_homework\", path of folder in drive, where you uploaded your homework\n","customized_path_to_homework = \"/content/drive/My Drive/DL_Fall_2020/Assignment_3\"\n","sys.path.append(customized_path_to_homework)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QsYndBrEWdNz","colab":{"base_uri":"https://localhost:8080/","height":221},"outputId":"f288f8be-2663-4b15-b160-46afc2d53c9f"},"source":["# run this to download dataset, give path to the download.sh file from your drive\n","!sh /content/drive/My\\ Drive/DL_Fall_2020/Assignment_3/download_data.sh"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2020-10-16 17:54:51--  http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\n","Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n","Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:80... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar [following]\n","--2020-10-16 17:54:52--  https://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\n","Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 460032000 (439M) [application/octet-stream]\n","Saving to: ‘VOCtrainval_06-Nov-2007.tar’\n","\n","rainval_06-Nov-2007   0%[                    ]   3.21M   234KB/s    eta 32m 37s^C\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LvKSKSTQWdN4"},"source":["# copy and place downloaded dataset to your drive. To access dataset multiple times, no need to download everytime you open colab.\n","!cp -r /content/VOCdevkit_2007 enter_path_to_your_folder_here (example: /content/drive/My\\ Drive/DL_Fall_2020/Assignment_3/)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8B_dXlD7WdN-"},"source":["import os\n","import random\n","import cv2\n","import numpy as np\n","import torch\n","from torch.utils.data import DataLoader\n","from torchvision import models\n","\n","from resnet_yolo import resnet50\n","from dataset import VocDetectorDataset\n","from eval_voc import evaluate\n","from predict import predict_image\n","from config import VOC_CLASSES, COLORS\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","%matplotlib inline\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xOnSQeQnWdOG"},"source":["## Initialization"]},{"cell_type":"code","metadata":{"id":"xLxYIMIiWdOH","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"2745721c-5add-405b-fa6a-e61e1008d48a"},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(torch.cuda.get_device_name()) # GPU name"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tesla P4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"W3Xv2d65WdON"},"source":["# You Only Look Once: Unified, Real-Time Object Detection \n","In this assignment, you need to implement the loss function and train the **YOLO object detector** (specfically, YOLO-v1). Here we provide a list of recommend readings for you:\n","- [YOLO original paper](https://arxiv.org/pdf/1506.02640.pdf) (recommended)\n","- [Object detection methods](http://slazebni.cs.illinois.edu/fall18/lec09_detection.pdf) (Slides)\n","- [Great post about YOLO](https://medium.com/adventures-with-deep-learning/yolo-v1-part-1-cfb47135f81f) on Medium\n","- [Differences between YOLO, YOLOv2 and YOLOv3\n","](https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088)\n","- [Great explanation of the Yolo Loss function](https://stats.stackexchange.com/questions/287486/yolo-loss-function-explanation)\n","\n","We adopt a variant of YOLO, which:\n","1. Use pretrained ResNet50 classifier as detector backbone. The pretrained model is offered in `torchvision.models`.\n","2. Instead of using a $7\\times7$ detection grid, we use $14\\times14$ to get a more finegrained detection.\n","\n","In general, the backbone models are usually pretrained on ImageNet dataset (> 1 million images) with numerous classes. As a result, having these pretrained backbone can greatly shorten the required training time, as well as improve the performance. <span style=\"color:red\">**But still, it takes at least 3-4 hours to train, not to mention that you might need to debug after one training run. So START EARLY, DON'T GO #YOLO!**</span>\n","\n","<img src=\"figure/example.png\" width=\"450\">\n","You are supposed to get a reasonable detector (like the ... above?) after training the model correctly."]},{"cell_type":"code","metadata":{"id":"pzZkuz8zWdOO"},"source":["# YOLO network hyperparameters\n","B = 2  # number of bounding box predictions per cell\n","S = 14  # width/height of network output grid (larger than 7x7 from paper since we use a different network)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"22esEq2uWdOT"},"source":["## Load the pretrained ResNet classifier\n","Load the pretrained classifier. By default, it would use the pretrained model provided by `Pytorch`."]},{"cell_type":"code","metadata":{"id":"BdHf7qmEWdOU","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a38bce84-d753-4900-8fe1-02b94c0ce029"},"source":["load_network_path = None\n","pretrained = True\n","\n","# use to load a previously trained network\n","if load_network_path is not None:\n","    print('Loading saved network from {}'.format(load_network_path))\n","    net = resnet50().to(device)\n","    net.load_state_dict(torch.load(load_network_path))\n","else:\n","    print('Load pre-trained model')\n","    net = resnet50(pretrained=pretrained).to(device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Load pre-trained model\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"r3ZErIubWdOY"},"source":["Some basic hyperparameter settings that you probably don't have to tune."]},{"cell_type":"code","metadata":{"id":"fBTbhwBQWdOZ"},"source":["learning_rate = 0.001\n","num_epochs = 20\n","batch_size = 10\n","\n","# Yolo loss component coefficients (as given in Yolo v1 paper)\n","lambda_coord = 5\n","lambda_noobj = 0.5"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qAumU6ZKWdOe"},"source":["## Implement the YOLO-v1 loss [80 pts]\n","Now, you have to implement the `YoloLoss` for training your object detector. Please read closely to the [YOLO original paper](https://arxiv.org/pdf/1506.02640.pdf) so that you can implement it.\n","\n","In general, there are 4 components in the YOLO loss. Consider that we have our prediction grid of size$(N, S, S, 5B+c)$ ( (x, y, w, h, C) for each bounding box, and c is the number of classes), where $N$ is the batch size, $S$ is the grid size, $B$ is the number of bounding boxes. We have :\n","1. Bounding box regression loss on the bounding box$(x, y, w, h)$\n","    - $l_{coord}=\\sum_{i=0}^{S^2}\\sum_{j=0}^B\\mathbb{1}^{obj}_{ij}\\left[(x_i-\\hat{x}_i)^2+(y_i-\\hat{y}_i)^2\\right]$ + $\\sum_{i=0}^{S^2}\\sum_{j=0}^B\\mathbb{1}^{obj}_{ij}\\left[(\\sqrt{w_i}-\\sqrt{\\hat{w}_i})^2+(\\sqrt{h_i}-\\sqrt{\\hat{h}_i})^2\\right]$\n","    - $\\mathbb{1}^{obj}_{ij}$: equals to 1 when object appears in cell $i$, and the bounding box $j$ is responsible for the prediction. 0 otherwise.\n","2. Contain object loss on the confidence prediction $c$ (only calculate for those boxes that actually have objects)\n","    - $l_{contain}=\\sum_{i=0}^{S^2}\\sum_{j=0}^B\\mathbb{1}^{obj}_{ij}(C_i-\\hat{C}_i)^2$\n","    - $C_i$ the predicted confidence score for cell $i$ from predicted box $j$\n","    - For each grid cell, you only calculate the contain object loss for the predicted bounding box that has maximum overlap (iou) with the gruond truth box.\n","    - We say that this predicted box with maximum iou is **responsible** for the prediction.\n","3. No object loss on the confidence prediction $c$ (only calculate for those boxes that don't have objects)\n","    - $l_{noobj}=\\sum_{i=0}^{S^2}\\sum_{j=0}^B\\mathbb{1}^{noobj}_{ij}(C_i-\\hat{C}_i)^2$\n","    - $\\mathbb{1}^{obj}_{ij}$: equals to 1 when **no object appears** in cell $i$.\n","4. Classification error loss.\n","    - $l_{class}=\\sum_{i=0}^{S^2}\\mathbb{1}_i^{obj}\\sum_{c\\in classes}\\left(p_i(c)-\\hat{p_i}(c)\\right)^2$\n","    - $p_i(c)$ is the predicted score for class $c$\n","    \n","Putting them together, we get the yolo loss:\n","\\begin{equation}\n","yolo=\\lambda_{coord}l_{coord}+l_{contain}+\\lambda_{noobj}l_{noobj}+l_{class}\n","\\end{equation}\n","where $\\lambda$ are hyperparameters. We have provided detailed comments to guide you through implementing the loss. So now, please complete the YoloLoss in the code block below. **If you have any problem with regard to implementation, post and discuss it on Piazza.**"]},{"cell_type":"code","metadata":{"id":"cjQjyhx3WdOf"},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n"," \n","class YoloLoss(nn.Module):\n","    def __init__(self,S,B,lambda_coord,lambda_noobj): #this was incorrectly assigned so I fixed it for future lines\n","        super(YoloLoss,self).__init__()\n","        self.S = S    # square grid size\n","        self.B = B    # number of bounding boxes per grid cell \n","        self.lambda_coord = lambda_coord     # localization penalty dampener/amplifier in loss function\n","        self.lambda_noobj = lambda_noobj     # detection penalty dampener/amplifier in loss function\n"," \n","    def compute_iou(self, box1, box2):                                                                                                                                                             \n","        \"\"\"Compute the intersection over union of two set of boxes, each box is [x1,y1,x2,y2].\n","        Args:\n","          box1: (tensor) bounding boxes, sized [N,4].\n","          box2: (tensor) bounding boxes, sized [M,4].\n","        Return:\n","          (tensor) iou, sized [N,M].\n","        \"\"\"\n","        N = box1.size(0)  # first size element of bounding box 1\n","        M = box2.size(0)  # first size element of bounding box 2\n","\n","        # Finds top left coordinate of each bounding box\n","        lt = torch.max(\n","            box1[:,:2].unsqueeze(1).expand(N,M,2),  # [N,2] -> [N,1,2] -> [N,M,2]\n","            box2[:,:2].unsqueeze(0).expand(N,M,2),  # [M,2] -> [1,M,2] -> [N,M,2]\n","        )   \n"," \n","        # Finds bottom right coordinate of each bounding box\n","        rb = torch.min(\n","            box1[:,2:].unsqueeze(1).expand(N,M,2),  # [N,2] -> [N,1,2] -> [N,M,2]\n","            box2[:,2:].unsqueeze(0).expand(N,M,2),  # [M,2] -> [1,M,2] -> [N,M,2]\n","        )   \n"," \n","        # finds the width and height of IoU\n","        wh = rb - lt  # [N,M,2] \n","        wh[wh<0] = 0  # clip at 0 (crop out negative values which imply image boundaries)\n","        inter = wh[:,:,0] * wh[:,:,1]  # [N,M] #Area of IoU\n"," \n","        area1 = (box1[:,2]-box1[:,0]) * (box1[:,3]-box1[:,1])  # [N,] # Area of box 1 (x2-x1)*(y2-y1)\n","        area2 = (box2[:,2]-box2[:,0]) * (box2[:,3]-box2[:,1])  # [M,] # Area of box 2 (x2-x1)*(y2-y1)\n","        area1 = area1.unsqueeze(1).expand_as(inter)  # [N,] -> [N,1] -> [N,M]\n","        area2 = area2.unsqueeze(0).expand_as(inter)  # [M,] -> [1,M] -> [N,M]\n"," \n","        iou = torch.true_divide(inter,(area1 + area2 - inter)) # IoU ratio, replaced division as it was deprecated\n","        return iou \n","    \n","    def get_class_prediction_loss(self, classes_pred, classes_target):\n","        \"\"\" \n","        Parameters:\n","        classes_pred : (tensor) size (batch_size, S, S, 20)                                                                                                                                        \n","        classes_target : (tensor) size (batch_size, S, S, 20)\n","         \n","        Returns:\n","        class_loss : scalar\n","        \"\"\"\n","        ##### CODE #####\n","        class_loss = nn.MSELoss(classes_pred,classes_target, reduction = 'sum') # Classification error loss, assuming classes_pred and classes_target are already in probability format \n","        ##### CODE #####\n","        return class_loss\n","         \n","         \n","    def get_regression_loss(self, box_pred_response, box_target_response):\n","        \"\"\"\n","        Parameters:\n","        box_pred_response : (tensor) size (-1, 5)\n","        box_target_response : (tensor) size (-1, 5)\n","        Note : -1 corresponds to ravels the tensor into the dimension specified \n","        See : https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view_as\n","         \n","        Returns:\n","        reg_loss : scalar\n","        \"\"\"\n","        ##### CODE #####\n","        # regresslion loss for localization (centerpoint offset penalty + height and width penalty)\n","        reg_loss = nn.MSELoss(box_pred_response[:,:2],box_target_response[:,:2],reduction='sum') + nn.MSELoss(torch.sqrt(box_pred_response[:,2:4]),torch.sqrt(box_target_response[:,2:4]),reduction='sum')\n","        ##### CODE #####\n","        return reg_loss\n","         \n","    def get_contain_object_loss(self, box_pred_response, box_target_response_iou):\n","        \"\"\"\n","        Parameters:\n","        box_pred_response : (tensor) size ( -1 , 5)\n","        box_target_response_iou : (tensor) size ( -1 , 5)\n","        Note : -1 corresponds to ravels the tensor into the dimension specified \n","        See : https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view_as\n","         \n","        Returns:\n","        contain_loss : scalar\n","        \"\"\"\n","        ##### CODE #####\n","        # object detection penalty\n","        contain_loss = MSELoss(box_pred_response[:,4],box_target_response_iou[:,4],reduction='sum') \n","        ##### CODE #####\n","        return contain_loss\n","         \n","    def get_no_object_loss(self, target_tensor, pred_tensor, no_object_mask):\n","        \"\"\"                                                                                                                                                                                        \n","        Parameters:\n","        target_tensor : (tensor) size (batch_size, S , S, 30)\n","        pred_tensor : (tensor) size (batch_size, S , S, 30)\n","        no_object_mask : (tensor) size (batch_size, S , S)\n","         \n","        Returns:\n","        no_object_loss : scalar\n","         \n","        Hints:\n","        1) Create 2 tensors no_object_prediction and no_object_target which only have the \n","        values which have no object. \n","        2) Have another tensor no_object_prediction_mask of the same size such that \n","        mask with respect to both confidences of bounding boxes set to 1. \n","        3) Create 2 tensors which are extracted from no_object_prediction and no_object_target using\n","        the mask created above to find the loss. \n","        \"\"\"\n","        ##### CODE #####\n","        # Tensor for values that have no object in prediction\n","        no_object_prediction = pred_tensor[no_object_mask].view(-1,30)\n","        # Tensor for values that have no object in actual data\n","        no_object_target = target_tensor[no_object_mask].view(-1,30)\n","        # Creating tensor to hold prediction mask which tells each layer which of the bounding boxes it is responsible for predicting\n","        no_object_prediction_mask = torch.cuda.ByteTensor(no_object_prediction.size())\n","        # Zeroing out created tensor\n","        no_object_prediction_mask.zero_()\n","        # Setting mask on relevant bounding boxes to 1\n","        no_object_prediction_mask[:,4]=1\n","        no_object_prediction_mask[:,9]=1\n","        # Applying mask to prediction and actual\n","        no_object_prediction_1 = no_object_prediction[no_object_prediction_mask]\n","        no_object_target_1 = no_object_target[no_object_prediction_mask]\n","        # Creating no object loss per grid cell\n","        no_object_loss = nn.MSELoss(no_object_prediction_1,no_object_target_1,reduction='sum')\n","        ##### CODE #####\n","        return no_object_loss\n","          \n","    def find_best_iou_boxes(self, box_target, box_pred):\n","        \"\"\"\n","        Parameters: \n","        box_target : (tensor)  size (-1, 5)\n","        box_pred : (tensor) size (-1, 5)\n","        Note : -1 corresponds to ravels the tensor into the dimension specified \n","        See : https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view_as\n","         \n","        Returns: \n","        box_target_iou: (tensor)\n","        contains_object_response_mask : (tensor)\n","         \n","        Hints:\n","        1) Find the iou's of each of the 2 bounding boxes of each grid cell of each image.\n","        2) Set the corresponding contains_object_response_mask of the bounding box with the max iou\n","        of the 2 bounding boxes of each grid cell to 1.\n","        3) For finding iou's use the compute_iou function (done)\n","        4) Before using compute preprocess the bounding box coordinates in such a way that \n","        if for a Box b the coordinates are represented by [x, y, w, h] then \n","        x, y = x/S - 0.5*w, y/S - 0.5*h ; w, h = x/S + 0.5*w, y/S + 0.5*h\n","        Note: Over here initially x, y are the center of the box and w,h are width and height. \n","        We perform this transformation to convert the correct coordinates into bounding box coordinates. (done)\n","        5) Set the confidence of the box_target_iou of the bounding box to the maximum iou (done)\n","        \"\"\"\n","        ##### CODE #####\n","        # Creating tensor to hold prediction mask which tells each layer which of the bounding boxes it is responsible for predicting\n","        contains_object_response_mask = torch.cuda.ByteTensor(box_target.size())\n","        # Zeroing out created tensor\n","        contains_object_response_mask.zero_()\n","        # Creating tensor to hold prediction mask which tells each layer which of the bounding boxes it is responsible for predicting\n","        not_contains_object_response_mask = torch.cuda.ByteTensor(box_target.size())\n","        # Zeroing out created tensor\n","        not_contains_object_response_mask.zero_()\n","        # Creating tensor to hold IoU\n","        box_target_iou = torch.zeros(box_target.size()).cuda()\n","        # for loop over each two proposed bounding boxes in each grid cell\n","        for i in range(0,box_pred.size(0),self.B): \n","            box_1 = box_pred[i:i+2] # proposed boxes in examined grid cell\n","            # create tensor to hold grid-relative coordinates (YOLO bounding box encoding)\n","            box_1_coords = Variable(torch.FloatTensor(box_1.size()))\n","            # Adjust centerpoint to be relative to grid\n","            box_1_coords[:,:2] = box_1[:,:2]/self.S -0.5*box_1[:,2:4]  # x, y\n","            box_1_coords[:,2:4] = box_1[:,:2]/self.S +0.5*box_1[:,2:4] # w, h\n","            # Reshape actual box to vector\n","            box_2 = box_target[i].view(-1,5)\n","            # create tensor to hold grid-relative coordinates (YOLO bounding box encoding)\n","            box_2_coords = Variable(torch.FloatTensor(box2.size()))\n","            # Adjust centerpoint to be relative to grid\n","            box_2_coords[:,:2] = box_2[:,:2]/self.S -0.5*box_2[:,2:4] # x, y\n","            box_2_coords[:,2:4] = box_2[:,:2]/self.S +0.5*box_2[:,2:4] # w, h\n","            iou = self.compute_iou(box_1_coords[:,:4],box_2_coords[:,:4]) # using previous fn to compute IoU ratio\n","            max_iou, max_index = iou.max(0)\n","            max_index = max_index.data.cuda() # convert to cuda for GPU computation\n","            # pick the box with the larger IoU ratio\n","            contains_object_response_mask[i+max_index]=1\n","            # pick box with smaller IoU ratio\n","            not_contains_object_response_mask[i+1-max_index]=1\n","            # Set IoUs for each true bounding box in the image\n","            box_target_iou[i+max_index,torch.LongTensor([4]).cuda()] = (max_iou).data.cuda()\n","        box_target_iou = Variable(box_target_iou).cuda()\n","        coo_response_mask = contains_object_response_mask\n","        ##### CODE #####\n","        return box_target_iou, coo_response_mask\n","         \n","    def forward(self, pred_tensor,target_tensor):\n","        '''\n","        pred_tensor: (tensor) size(batchsize,S,S,Bx5+20=30)\n","                      where B - number of bounding boxes this grid cell is a part of = 2\n","                            5 - number of bounding box values corresponding to [x, y, w, h, c]\n","                                where x - x_coord, y - y_coord, w - width, h - height, c - confidence of having an object\n","                            20 - number of classes\n","         \n","        target_tensor: (tensor) size(batchsize,S,S,30)\n","         \n","        Returns:\n","        Total Loss\n","        '''\n","        N = pred_tensor.size(0) # size of input\n","         \n","        total_loss = None #initialization of variable\n","        # Create 2 tensors contains_object_mask and no_object_mask \n","        # of size (Batch_size, S, S) such that each value corresponds to if the confidence of having \n","        # an object > 0 in the target tensor.\n","\n","        ##### CODE #####\n","        contains_object_mask = target_tensor[:,:,:,4] > 0\n","        no_object_mask = target_tensor[:,:,:,4] == 0\n","        contains_object_mask = contains_object_mask.unsqueeze(-1).expand_as(target_tensor)\n","        no_object_mask = no_object_mask.unsqueeze(-1).expand_as(target_tensor)\n","        ##### CODE #####                      \n","        \n","        \"\"\"\n","        Create a tensor contains_object_pred that corresponds to \n","        to all the predictions which seem to confidence > 0 for having an object\n","        Then, split this tensor into 2 tensors :                                                                                                                                                       \n","        1) bounding_box_pred : Contains all the Bounding box predictions (x, y, w, h, c) of all grid \n","                                cells of all images\n","        2) classes_pred : Contains all the class predictions for each grid cell of each image\n","        Hint : Use contains_object_mask\n","        \"\"\" \n","        ##### CODE #####\n","        contains_object_pred = pred_tensor[contains_object_mask].view(-1,30)\n","        bounding_box_pred = contains_object_pred[:,:10].contiguous().view(-1,5) \n","        classes_pred = contains_object_pred[:,10:] \n","        ##### CODE #####                   \n","        \"\"\"\n","        # Similarly, create 2 tensors bounding_box_target and classes_target\n","        # using the contains_object_mask.\n","        \"\"\"\n","        ##### CODE #####\n","        contains_object_target = target_tensor[contains_object_mask].view(-1,30)\n","        bounding_box_target = contains_object_target[:,:10].contiguous().view(-1,5)\n","        classes_target = contains_object_target[:,10:]\n","        ##### CODE #####\n","        \n","        #Compute the No object loss here\n","        # Instruction: finish your get_no_object_loss\n","        ##### CODE #####\n","        no_object_loss = self.get_no_object_loss(target_tensor, pred_tensor, no_object_mask)\n","        ##### CODE #####\n","        \"\"\"\n","        # Compute the iou's of all bounding boxes and the mask for which bounding box \n","        # of 2 has the maximum iou the bounding boxes for each grid cell of each image.\n","        # Instruction: finish your find_best_iou_boxes and use it.\n","        \"\"\"\n","        ##### CODE #####\n","        best_iou_boxes = self.find_best_iou_boxes(bounding_box_target, bounding_box_pred)[0]\n","        ##### CODE #####\n","        \"\"\"        \n","        # Create 3 tensors :\n","        # 1) box_prediction_response - bounding box predictions for each grid cell which has the maximum iou\n","        # 2) box_target_response_iou - bounding box target ious for each grid cell which has the maximum iou\n","        # 3) box_target_response -  bounding box targets for each grid cell which has the maximum iou\n","        # Hint : Use coo_response_mask\n","        \"\"\"\n","        ##### CODE #####\n","        box_prediction_response = bounding_box_pred[self.find_best_iou_boxes(bounding_box_target, bounding_box_pred)[1]].view(-1,5)\n","        box_target_response_iou = best_iou_boxes[self.find_best_iou_boxes(bounding_box_target, bounding_box_pred)[1]].view(-1,5)\n","        box_target_response = bounding_box_target[self.find_best_iou_boxes(bounding_box_target, bounding_box_pred)[1]].view(-1,5) \n","        ##### CODE #####\n","        \"\"\"\n","        # Find the class_loss, containing object loss and regression loss\n","        \"\"\"\n","        ##### CODE #####\n","        # computing final pieces of loss function\n","        localization_loss = self.lambda_coord*(self.get_regression_loss(box_prediction_response, box_target_response))\n","        detection_loss = self.get_contain_object_loss(box_prediction_response, box_target_response_iou)+self.lambda_noobj*self.get_no_object_loss(target_tensor, pred_tensor, no_object_mask)\n","        class_prediction_loss = self.get_class_prediction_loss(classes_pred, classes_target)\n","        total_loss = localization_loss + detection_loss + class_prediction_loss\n","        ##### CODE #####\n","        return total_loss / N "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Ik61zV3WdOk"},"source":["criterion = YoloLoss(S, B, lambda_coord, lambda_noobj)\n","optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XU3JE7IlWdOp"},"source":["## Reading Pascal Data"]},{"cell_type":"markdown","metadata":{"id":"WduG4TFnWdOq"},"source":["Since Pascal is a small dataset (5000 in train+val) we have combined the train and val splits to train our detector. This is not typically a good practice, but we will make an exception in this case to be able to get reasonable detection results with a comparatively small object detection dataset. Use `download_data.sh` to download the dataset.\n","\n","The train dataset loader also using a variety of data augmentation techniques including random shift, scaling, crop, and flips. Data augmentation is slightly more complicated for detection dataset since the bounding box annotations must be kept consistent through the transformations.\n","\n","Since the output of the dector network we train is a $(S, S, 5B+c)$ tensor, we use an encoder to convert the original bounding box coordinates into relative grid bounding box coordinates corresponding to the the expected output. We also use a decoder which allows us to convert the opposite direction into image coordinate bounding boxes."]},{"cell_type":"code","metadata":{"id":"SjQo5AEAWdOr","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"d3309c46-0d43-4ed8-e06e-21763d95bcff"},"source":["file_root_train = customized_path_to_homework + '/VOCdevkit_2007/VOC2007/JPEGImages/'\n","annotation_file_train = customized_path_to_homework + '/voc2007.txt'\n","\n","train_dataset = VocDetectorDataset(root_img_dir=file_root_train,dataset_file=annotation_file_train,train=True, S=S)\n","train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True,num_workers=4)\n","print('Loaded %d train images' % len(train_dataset))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Initializing dataset\n","Loaded 5011 train images\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PvicZLodWdOw","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"7e5f20b8-f64c-4eef-994e-e9528f28f3f5"},"source":["file_root_test = customized_path_to_homework + '/VOCdevkit_2007/VOC2007test/JPEGImages/'\n","annotation_file_test = customized_path_to_homework + '/voc2007test.txt'\n","\n","test_dataset = VocDetectorDataset(root_img_dir=file_root_test,dataset_file=annotation_file_test,train=False, S=S)\n","test_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False,num_workers=4)\n","print('Loaded %d test images' % len(test_dataset))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Initializing dataset\n","Loaded 4950 test images\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1j9iEjzPWdO0"},"source":["## Train detector\n","Now, train your detector."]},{"cell_type":"code","metadata":{"id":"xnQtQK4RWdO1","colab":{"base_uri":"https://localhost:8080/","height":530},"outputId":"6aa1d41f-428d-4d35-ec86-29e256629070"},"source":["best_test_loss = np.inf\n","\n","for epoch in range(num_epochs):\n","    net.train()\n","    \n","    # Update learning rate late in training\n","    if epoch == 30 or epoch == 40:\n","        learning_rate /= 10.0\n","\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = learning_rate\n","    \n","    print('\\n\\nStarting epoch %d / %d' % (epoch + 1, num_epochs))\n","    print('Learning Rate for this epoch: {}'.format(learning_rate))\n","    \n","    total_loss = 0.\n","    \n","    for i, (images, target) in enumerate(tqdm(train_loader, total=len(train_loader))):\n","        images, target = images.to(device), target.to(device)\n","\n","        pred = net(images)\n","        loss = criterion(pred,target)\n","        total_loss += loss.item()\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    print('Epoch [%d/%d], average_loss: %.4f'\n","            % (epoch+1, num_epochs, total_loss / (i+1)))\n","    \n","    # evaluate the network on the test data\n","    with torch.no_grad():\n","        test_loss = 0.0\n","        net.eval()\n","        for i, (images, target) in enumerate(tqdm(test_loader, total=len(test_loader))):\n","            images, target = images.to(device), target.to(device)\n","\n","            pred = net(images)\n","            loss = criterion(pred,target)\n","            test_loss += loss.item()\n","        test_loss /= len(test_loader)\n","    \n","    if best_test_loss > test_loss:\n","        best_test_loss = test_loss\n","        print('Updating best test loss: %.5f' % best_test_loss)\n","        torch.save(net.state_dict(),'best_detector.pth')\n","\n","    torch.save(net.state_dict(),'detector.pth')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/502 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\n","Starting epoch 1 / 20\n","Learning Rate for this epoch: 0.001\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:129: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:20.)\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:130: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:20.)\n","  0%|          | 0/502 [00:04<?, ?it/s]\n"],"name":"stderr"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-ddde3e54fc43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-fcb026de2372>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pred_tensor, target_tensor)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;31m# Instruction: finish your get_no_object_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;31m##### CODE #####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0mno_object_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_no_object_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_object_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0;31m##### CODE #####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \"\"\"\n","\u001b[0;32m<ipython-input-9-fcb026de2372>\u001b[0m in \u001b[0;36mget_no_object_loss\u001b[0;34m(self, target_tensor, pred_tensor, no_object_mask)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mno_object_target_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mno_object_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mno_object_prediction_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;31m# Creating no object loss per grid cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mno_object_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_object_prediction_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mno_object_target_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;31m##### CODE #####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mno_object_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py\u001b[0m in \u001b[0;36mlegacy_get_string\u001b[0;34m(size_average, reduce, emit_warning)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mreduce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"]}]},{"cell_type":"markdown","metadata":{"id":"e65w9rFMWdO5"},"source":["# View example predictions"]},{"cell_type":"markdown","metadata":{"id":"idHHHdRDWdO6"},"source":["Now, take a glance at how your detector works:"]},{"cell_type":"code","metadata":{"id":"3ppbgq8pWdO7"},"source":["net.eval()\n","net.load_state_dict(torch.load('best_detector.pth'))\n","# select random image from train set\n","image_name = random.choice(train_dataset.fnames)\n","image = cv2.imread(os.path.join(file_root_train, image_name))\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","threshold = 0.1\n","print('predicting...')\n","print(image.shape)\n","result = predict_image(net, image_name, root_img_directory=file_root_train, threshold=threshold)\n","for left_up, right_bottom, class_name, _, prob in result:\n","    color = COLORS[VOC_CLASSES.index(class_name)]\n","    cv2.rectangle(image, left_up, right_bottom, color, 2)\n","    label = class_name + str(round(prob, 2))\n","    text_size, baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.4, 1)\n","    p1 = (left_up[0], left_up[1] - text_size[1])\n","    cv2.rectangle(image, (p1[0] - 2 // 2, p1[1] - 2 - baseline), (p1[0] + text_size[0], p1[1] + text_size[1]),\n","                  color, -1)\n","    cv2.putText(image, label, (p1[0], p1[1] + baseline), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1, 8)\n","\n","plt.figure(figsize = (15,15))\n","plt.imshow(image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"1EOOpIXPWdPA"},"source":["## Evaluate on Test [20 pts]\n","\n","To evaluate detection results we use mAP (mean of average precision over each class), You are expected to get an map of at least 49."]},{"cell_type":"code","metadata":{"id":"vnwmbwGFWdPB"},"source":["test_aps = evaluate(net, test_dataset_file=annotation_file_test, threshold=threshold)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6IGClvi7WdPF"},"source":["### Guidelines for Downloading PDF in Google Colab\n","- Run below cells only in Google Colab, Comment out in case of Jupyter notebook"]},{"cell_type":"code","metadata":{"id":"2wV-d0zwWdPG"},"source":["#Run below two lines (in google colab), installation steps to get .pdf of the notebook\n","\n","!apt-get install texlive texlive-xetex texlive-latex-extra pandoc\n","!pip install pypandoc\n","\n","# After installation, comment above two lines and run again to remove installation comments from the notebook."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LQWwJy_GWdPK"},"source":["# Find path to your notebook file in drive and enter in below line\n","\n","!jupyter nbconvert --to PDF \"your_notebook_path_here/DL_Assignment_2.ipynb\"\n","\n","#Example: \"/content/drive/My Drive/DL_Fall_2020/Assignment_2/DL_Assignment_2.ipynb\""],"execution_count":null,"outputs":[]}]}